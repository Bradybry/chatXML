{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt-Busters Entry Number 2 - Chained Chain of Thought Prompting\n",
    "<p style=\"margin-top: -20px; font-size: 0.8em;\">By Bryce Brady</p>\n",
    "\n",
    "In PB-1 we explored single inference chain of thought prompting. One question I have wanted to answer but not seen any discussion on is whether or not chain of thought is better when it involves actual chains of prompts. For example, instead of an single prompt in which an LLM explains their reasoning and then provides an answer, you coulde request only the reasoning and then make a second call in which the answer is determined. In essence, by doing both at once we are actually requesting that the model do two tasks, despite their inter-dependence. Generally, increasing the number of tasks requested in a prompt decreases performance. Is the same true about chain of prompt?\n",
    "\n",
    "### Chained Chain of Thought Hypothesis\n",
    "Chain-of-thought prompting is a technique to elicit reasoning from large language models. It involves prompting the model with examples that demonstrate a chain of reasoning for solving a task, like a multi-step math word problem. The examples provide a series of natural language intermediate steps that show how to decompose the problem and arrive at the final solution.\n",
    "\n",
    "By performing chain of thought through a chain of prompts. The first requests just the chain of thought reasoning and the second requests a determination of the answer, we can reduce the \"cognitive load\" on the model and improve performance. \n",
    "\n",
    "Something we will not be testing but is a secondary benefit for chained chain of thought is that we also guarantee an answer even if we run out of reasoning completion tokens since the answer inference is always performed separetely. Of course, it will be doing inference on an incomplete reasoning chain which may not be ideal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "To assess whether chained chain-of-thought prompting improves AI reasoning abilities, we need a challenging dataset and model for testing different prompts.\n",
    "\n",
    "### Dataset: ABA Model Rules of Professional Conduct Questions\n",
    "\n",
    "We will use the *aba_MRPC_true_false* evaluation dataset from Anthropic containing 110 True/False questions on legal ethics based on the American Bar Association (ABA) Model Rules of Professional Conduct. For example:\n",
    "\n",
    "> Question: Newly admitted lawyers cannot be as competent as practitioners with long experience.  \n",
    "> Answer: False\n",
    "\n",
    "One hundred questions will be used for testing prompts, reserving 10 for potential future one-shot or few-shot learning evaluations. One hundred samples allows for an interpretable results analysis while posing a reasonable challenge for the AI model.  \n",
    "\n",
    "### Model: Claude by Anthropic\n",
    "\n",
    "We will test prompts using Claude-v1.3, an AI assistant created by Anthropic, due to factors including its reasonable API pricing and speeds, and service reliability.\n",
    "\n",
    "PS I can't afford to do this with GPT-4 even if their API worked better.\n",
    "\n",
    "### Prompt Design\n",
    "\n",
    "Two types of prompts will be evaluated: a \"null prompt\" providing only the basic instructions and a \"chain of thought prompt\" encouraging step-by-step reasoning.\n",
    "\n",
    "#### Null Prompt:\n",
    "Note: This is identical to the PB-1 COT prompt. So we should expect the same performance since temp = 0. Stay tuned to see how that works out...\n",
    "```xml\n",
    "\n",
    "\n",
    "Assistant: <assistant_instructions>\n",
    "    <role>\n",
    "        Correctly Answer True/False questions about the ABA Model Rules of Professional Conduct. Before providing an answer, think through each step of your reasoning.\n",
    "    </role>\n",
    "    <output_format>\n",
    "        <reasoning>\n",
    "            {scratch pad for reasoning through the question}\n",
    "        </reasoning>\n",
    "        <answer>\n",
    "            {True or False}\n",
    "        </answer>\n",
    "    </output_format>\n",
    "</assistant_instructions>\n",
    "\n",
    "\n",
    "Human:  <input>\n",
    "    {True or False Question to Answer}\n",
    "</input>\n",
    "\n",
    "\n",
    "Assistant: <output>\n",
    "    <reasoning>...\n",
    "        \n",
    "```\n",
    "\n",
    "#### Alternative Prompt:\n",
    "\n",
    "Step 1:\n",
    "```xml\n",
    "\n",
    "\n",
    "Assistant: <assistant_instructions>\n",
    "    <role>\n",
    "        Correctly Answer True/False questions about the ABA Model Rules of Professional Conduct. Think through each step of your reasoning. Only provided your reasoning do not provide a final determination yet.\n",
    "    </role>\n",
    "    <output_format>\n",
    "        <reasoning>\n",
    "            {scratch pad for reasoning through the question}\n",
    "        </reasoning>\n",
    "    </output_format>\n",
    "</assistant_instructions>\n",
    "\n",
    "\n",
    "Human:  <input>\n",
    "    {True or False Question to Answer}\n",
    "</input>\n",
    "\n",
    "\n",
    "Assistant: <output>\n",
    "    <reasoning>...\n",
    "        \n",
    "```\n",
    "\n",
    "Step 2:\n",
    "```xml\n",
    "Assistant: <assistant_instructions>\n",
    "    <role>\n",
    "        Correctly Answer True/False questions about the ABA Model Rules of Professional Conduct. Use the question and the provided reasoning to make a determination. Only provide your determination of the answer.\n",
    "    </role>\n",
    "    <output_format>\n",
    "        <answer>\n",
    "            {True or False}\n",
    "        </answer>\n",
    "    </output_format>\n",
    "</assistant_instructions>\n",
    "\n",
    "\n",
    "Human:  <input>\n",
    "    <question>\n",
    "        {True of False Question to Answer}\n",
    "    </question>\n",
    "    <reasoning>\n",
    "        {Expert's reasoning about what the correction answer may be}\n",
    "    </reasoning>\n",
    "</input>\n",
    "\n",
    "\n",
    "Assistant: <output>\n",
    "    <answer>...\n",
    "        \n",
    "```\n",
    "\n",
    "### Evaluation Methodology \n",
    "\n",
    "#### Scoring\n",
    "\n",
    "1. **F1 Score** - *The harmonic mean of precision and recall, measuring the accuracy of the model.*\n",
    "2. **Average Number of Prompt Tokens** - *The mean number of tokens (words and punctuation) in prompts generated.*  \n",
    "3. **Average Number of Completion Tokens** - *The mean number of tokens in the model's completions.*\n",
    "4. **Average Latency** - *The mean time in seconds for the model to generate a completion.*\n",
    "\n",
    "To determine if the alternative prompt technique is **Confirmed** or **Busted** based solely on performance, we will compare F1 scores and perform the McNemar's test. If the alternative prompt is significantly different from the null prompt AND performance is improved, it will be considered **Confirmed**. If the the null prompt meets of outperforms the alternative prompt, the technique will be **Busted**. A technique is **Plausible** if we get a better a F1 score but do not have signficant results based on the McNemar's test.\n",
    "\n",
    "McNemar's test is a non-parametric statistical test used to compare the performance of two classifiers on the same dataset. It's particularly useful when dealing with paired nominal data, like the predictions of two classifiers for the same set of instances. The test is based on the assumption that the two classifiers have the same error rates, and it evaluates if there is a significant difference in the number of correct and incorrect classifications made by each classifier.\n",
    "\n",
    "The additional metrics of prompt length, completion length, and latency will also be compared to check for any increase in cost or decrease in efficiency from a technique, even if accuracy improves. A **Confirmed** prompt that drastically impacts these secondary metrics may still not be optimal or practical for real-world use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports and helper functions\n",
    "import json\n",
    "import pandas as pd\n",
    "from langchain.llms import Anthropic\n",
    "from config import ANTHROPIC_API_KEY\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import tiktoken\n",
    "from sklearn.metrics import classification_report\n",
    "llm = Anthropic(model=\"claude-v1.3\", temperature=0.0, max_tokens_to_sample=512, anthropic_api_key=ANTHROPIC_API_KEY)\n",
    "tqdm.pandas()\n",
    "def generate_null_prompt(messages):\n",
    "    question = messages[1]['content']   \n",
    "    xml = '''\\n\\nAssistant: <assistant_instructions>\n",
    "    <role>\n",
    "        Correctly Answer True/False questions about the ABA Model Rules of Professional Conduct. Before providing an answer, think through each step of your reasoning.\n",
    "    </role>\n",
    "    <output_format>\n",
    "        <reasoning>\n",
    "            {scratch pad for reasoning through the question}\n",
    "        </reasoning>\n",
    "        <answer>\n",
    "            {True or False}\n",
    "        </answer>\n",
    "    </output_format>\n",
    "</assistant_instructions>\\n\\nHuman: <input>\n",
    "    '''\n",
    "    xml += question\n",
    "    xml += '''\n",
    "</input>\\n\\nAssistant: <output>\n",
    "    <reasoning>'''\n",
    "    return xml\n",
    "\n",
    "def generate_alt_prompt(messages):\n",
    "    question = messages[1]['content']   \n",
    "    xml = '''\\n\\nAssistant: <assistant_instructions>\n",
    "    <role>\n",
    "        Correctly Answer True/False questions about the ABA Model Rules of Professional Conduct. Think through each step of your reasoning. Only provide your reasoning do not provide a final determination yet.\n",
    "    </role>\n",
    "    <output_format>\n",
    "        <reasoning>\n",
    "            {scratch pad for reasoning through the question}\n",
    "        </reasoning>\n",
    "    </output_format>\n",
    "</assistant_instructions>\\n\\nHuman: <input>\n",
    "    '''\n",
    "    xml += question\n",
    "    xml += '''\n",
    "</input>\\n\\nAssistant: <output>\n",
    "    <reasoning>'''\n",
    "    return xml\n",
    "def generate_alt_prompt2(question, alt_result_1):  \n",
    "    xml = '''\\n\\nAssistant: <assistant_instructions>\n",
    "    <role>\n",
    "        Correctly Answer True/False questions about the ABA Model Rules of Professional Conduct. Use the question and the provided reasoning to make a determination. Only provide your determination of the answer.\n",
    "    </role>\n",
    "    <outupt_format>\n",
    "        <answer>\n",
    "            {True or False}\n",
    "        </answer>\n",
    "    </outupt_format>\n",
    "</assistant_instructions>\\n\\nHuman: <input>\n",
    "    <question>\n",
    "        '''\n",
    "    xml += question\n",
    "    xml += '''\n",
    "    </question>\n",
    "    <reasoning>\n",
    "        '''\n",
    "    xml += alt_result_1\n",
    "    xml += '''\n",
    "</input>\\n\\nAssistant: <output>\n",
    "    <answer>'''\n",
    "    return xml\n",
    "\n",
    "def extract(result):\n",
    "    result = result.lower()\n",
    "    if 'true' in result:\n",
    "        return 'True'\n",
    "    elif 'false' in result:\n",
    "        return 'False'\n",
    "    \n",
    "def get_alt_result(alt_input_1, llm):\n",
    "    alt_result_1 = llm(alt_input_1)    \n",
    "    if '<reasoning>' in alt_result_1:\n",
    "        alt_result_1_reasoning = alt_result_1.split('<reasoning>')[1]\n",
    "    else:\n",
    "        alt_result_1_reasoning = alt_result_1\n",
    "    if '</reasoning>' in alt_result_1_reasoning:\n",
    "        alt_result_1_reasoning = alt_result_1_reasoning.split('</reasoning>')[0].strip()\n",
    "    question = alt_input_1.split('<input>')[1].split('</input>')[0].strip()\n",
    "    alt_input_2 = generate_alt_prompt2(question, alt_result_1_reasoning)\n",
    "    alt_result_2 = llm(alt_input_2)\n",
    "    return alt_result_1, alt_input_2, alt_result_2\n",
    "target_names = {0: 'False', 1: 'True'}\n",
    "target_numbers = {v: k for k, v in target_names.items()}\n",
    "\n",
    "## We will estimate tokens using the OpenAI Ada encoding. Not perfect but probably good enough.\n",
    "embedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\n",
    "encoding = tiktoken.get_encoding(embedding_encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>ideal</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>null_input</th>\n",
       "      <th>alt_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are LawStu...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\nAssistant: &lt;assistant_instructions&gt;\\n    &lt;...</td>\n",
       "      <td>\\n\\nAssistant: &lt;assistant_instructions&gt;\\n    &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are LawStu...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\nAssistant: &lt;assistant_instructions&gt;\\n    &lt;...</td>\n",
       "      <td>\\n\\nAssistant: &lt;assistant_instructions&gt;\\n    &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are LawStu...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\nAssistant: &lt;assistant_instructions&gt;\\n    &lt;...</td>\n",
       "      <td>\\n\\nAssistant: &lt;assistant_instructions&gt;\\n    &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are LawStu...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\nAssistant: &lt;assistant_instructions&gt;\\n    &lt;...</td>\n",
       "      <td>\\n\\nAssistant: &lt;assistant_instructions&gt;\\n    &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are LawStu...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\nAssistant: &lt;assistant_instructions&gt;\\n    &lt;...</td>\n",
       "      <td>\\n\\nAssistant: &lt;assistant_instructions&gt;\\n    &lt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  ideal  ground_truth   \n",
       "0  [{'role': 'system', 'content': 'You are LawStu...  False             0  \\\n",
       "1  [{'role': 'system', 'content': 'You are LawStu...  False             0   \n",
       "2  [{'role': 'system', 'content': 'You are LawStu...   True             1   \n",
       "3  [{'role': 'system', 'content': 'You are LawStu...  False             0   \n",
       "4  [{'role': 'system', 'content': 'You are LawStu...  False             0   \n",
       "\n",
       "                                          null_input   \n",
       "0  \\n\\nAssistant: <assistant_instructions>\\n    <...  \\\n",
       "1  \\n\\nAssistant: <assistant_instructions>\\n    <...   \n",
       "2  \\n\\nAssistant: <assistant_instructions>\\n    <...   \n",
       "3  \\n\\nAssistant: <assistant_instructions>\\n    <...   \n",
       "4  \\n\\nAssistant: <assistant_instructions>\\n    <...   \n",
       "\n",
       "                                           alt_input  \n",
       "0  \\n\\nAssistant: <assistant_instructions>\\n    <...  \n",
       "1  \\n\\nAssistant: <assistant_instructions>\\n    <...  \n",
       "2  \\n\\nAssistant: <assistant_instructions>\\n    <...  \n",
       "3  \\n\\nAssistant: <assistant_instructions>\\n    <...  \n",
       "4  \\n\\nAssistant: <assistant_instructions>\\n    <...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load and format data\n",
    "file = '../data/aba_MRPC_true_false.jsonl'\n",
    "with open(file) as f:\n",
    "    lines = f.readlines()\n",
    "    lines = [json.loads(line) for line in lines]\n",
    "\n",
    "df = pd.DataFrame(lines)\n",
    "df['ground_truth'] = df.ideal.map(target_numbers)\n",
    "df['null_input'] = df.apply(lambda x: generate_null_prompt(x['input']), axis=1)\n",
    "df['alt_input'] = df.apply(lambda x: generate_alt_prompt(x['input']), axis=1) # cant generation alt_input2 yet\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Prompt:\n",
      "\n",
      "\n",
      "\n",
      "Assistant: <assistant_instructions>\n",
      "    <role>\n",
      "        Correctly Answer True/False questions about the ABA Model Rules of Professional Conduct. Before providing an answer, think through each step of your reasoning.\n",
      "    </role>\n",
      "    <output_format>\n",
      "        <reasoning>\n",
      "            {scratch pad for reasoning through the question}\n",
      "        </reasoning>\n",
      "        <answer>\n",
      "            {True or False}\n",
      "        </answer>\n",
      "    </output_format>\n",
      "</assistant_instructions>\n",
      "\n",
      "Human: <input>\n",
      "    A lawyer with general experience not considered competent to handle a case involving a specialized field of law.\n",
      "</input>\n",
      "\n",
      "Assistant: <output>\n",
      "    <reasoning>\n",
      "\n",
      "\n",
      "alt Prompt:\n",
      "\n",
      "\n",
      "\n",
      "Assistant: <assistant_instructions>\n",
      "    <role>\n",
      "        Correctly Answer True/False questions about the ABA Model Rules of Professional Conduct. Think through each step of your reasoning. Only provide your reasoning do not provide a final determination yet.\n",
      "    </role>\n",
      "    <output_format>\n",
      "        <reasoning>\n",
      "            {scratch pad for reasoning through the question}\n",
      "        </reasoning>\n",
      "    </output_format>\n",
      "</assistant_instructions>\n",
      "\n",
      "Human: <input>\n",
      "    A lawyer with general experience not considered competent to handle a case involving a specialized field of law.\n",
      "</input>\n",
      "\n",
      "Assistant: <output>\n",
      "    <reasoning>\n"
     ]
    }
   ],
   "source": [
    "## 1. Check input format\n",
    "print(\"Null Prompt:\\n\")\n",
    "print(df.null_input[0])\n",
    "\n",
    "print(\"\\n\\nalt Prompt:\\n\")\n",
    "print(df.alt_input[0])\n",
    "\n",
    "\n",
    "## 2. Drop first ten rows\n",
    "df = df.iloc[10:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Prompt Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [06:49<00:00,  4.09s/it]\n"
     ]
    }
   ],
   "source": [
    "## Generate Null Prompt Results\n",
    "start_time = time.time()\n",
    "df['null_result'] = df.progress_apply(lambda x: llm(x['null_input']), axis=1)\n",
    "time_per_it = (time.time() - start_time)/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pull out answers\n",
    "## TODO - this is a hacky way to do this. Should be a better way.\n",
    "df['null_answer'] = df.apply(lambda x: extract(x['null_result']), axis=1)\n",
    "df['null_pred'] = df.null_answer.map(target_numbers)\n",
    "\n",
    "## Get token counts\n",
    "df[\"null_completion_tokens\"] = df.null_result.apply(lambda x: len(encoding.encode(x)))\n",
    "df[\"null_prompt_tokens\"] = df.null_input.apply(lambda x: len(encoding.encode(x)))\n",
    "\n",
    "## Calculate performance stats\n",
    "y_true = df['ground_truth']\n",
    "y_pred = df['null_pred']\n",
    "report = classification_report(y_true, y_pred,target_names=target_names.values(), labels=list(target_names.keys()), zero_division=1, output_dict=True)\n",
    "\n",
    "### Collect statistics\n",
    "null_stats = {\n",
    "    \"precision\": report['weighted avg']['precision'],\n",
    "    \"recall\": report['weighted avg']['recall'],\n",
    "    \"f1\":  report['weighted avg']['f1-score'],\n",
    "    \"support\": report['weighted avg']['support'],\n",
    "    \"completion_tokens\": df[\"null_completion_tokens\"].mean(),\n",
    "    \"prompt_tokens\": df[\"null_prompt_tokens\"].mean(),\n",
    "    \"latency\": time_per_it\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.768082368082368,\n",
       " 'recall': 0.77,\n",
       " 'f1': 0.7683243585682611,\n",
       " 'support': 100,\n",
       " 'completion_tokens': 129.51,\n",
       " 'prompt_tokens': 139.99,\n",
       " 'latency': 4.090571904182434}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Prompt Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [10:36<00:00,  6.37s/it]\n"
     ]
    }
   ],
   "source": [
    "## Generate Alternative Prompt Results\n",
    "start_time = time.time()\n",
    "df['alt_result_1'], df['alt_input_2'], df['alt_result_2'] = zip(*df.progress_apply(lambda x: get_alt_result(x['alt_input'], llm), axis=1))\n",
    "time_per_it = (time.time() - start_time)/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pull out answers\n",
    "## TODO - this is a hacky way to do this. Should be a better way.\n",
    "df['alt_answer'] = df.apply(lambda x: extract(x['alt_result_2']), axis=1)\n",
    "df['alt_pred'] = df.alt_answer.map(target_numbers)\n",
    "\n",
    "## Get token counts\n",
    "df[\"alt_completion_tokens\"] = df.alt_result_1.apply(lambda x: len(encoding.encode(x))) + df.alt_result_2.apply(lambda x: len(encoding.encode(x)))\n",
    "df[\"alt_prompt_tokens\"] = df.alt_input.apply(lambda x: len(encoding.encode(x))) + df.alt_input_2.apply(lambda x: len(encoding.encode(x)))\n",
    "\n",
    "## Calculate performance stats\n",
    "y_true = df['ground_truth']\n",
    "y_pred = df['alt_pred']\n",
    "report = classification_report(y_true, y_pred,target_names=target_names.values(), labels=list(target_names.keys()), zero_division=1, output_dict=True)\n",
    "\n",
    "### Collect statistics\n",
    "alt_stats = {\n",
    "    \"precision\": report['weighted avg']['precision'],\n",
    "    \"recall\": report['weighted avg']['recall'],\n",
    "    \"f1\":  report['weighted avg']['f1-score'],\n",
    "    \"support\": report['weighted avg']['support'],\n",
    "    \"completion_tokens\": df[\"alt_completion_tokens\"].mean(),\n",
    "    \"prompt_tokens\": df[\"alt_prompt_tokens\"].mean(),\n",
    "    \"latency\": time_per_it\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.7576388888888889,\n",
       " 'recall': 0.76,\n",
       " 'f1': 0.7575551782682513,\n",
       " 'support': 100,\n",
       " 'completion_tokens': 187.65,\n",
       " 'prompt_tokens': 448.47,\n",
       " 'latency': 6.368724050521851}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>support</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alt_stats</th>\n",
       "      <td>0.757639</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.757555</td>\n",
       "      <td>100.0</td>\n",
       "      <td>187.65</td>\n",
       "      <td>448.47</td>\n",
       "      <td>6.368724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>null_stats</th>\n",
       "      <td>0.768082</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.768324</td>\n",
       "      <td>100.0</td>\n",
       "      <td>129.51</td>\n",
       "      <td>139.99</td>\n",
       "      <td>4.090572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Let's display a table of the stats before doing any analysis.\n",
    "from IPython.display import display, HTML, Markdown\n",
    "data = {\"alt_stats\": alt_stats, \"null_stats\": null_stats}\n",
    "results = pd.DataFrame(data).transpose()\n",
    "\n",
    "# Display the HTML table\n",
    "display(HTML(results.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "McNemar's test p-value: 1.0\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "# Perform McNemar's test\n",
    "contingency_table = pd.crosstab(df['null_pred'] == df['ground_truth'], df['alt_pred'] == df['ground_truth'])\n",
    "result = mcnemar(contingency_table, exact=True)\n",
    "\n",
    "print(f\"McNemar's test p-value: {result.pvalue}\")\n",
    "if result.pvalue < 0.05: sig = \"\" \n",
    "else: sig = \"not\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>null_completion_tokens</th>\n",
       "      <th>alt_completion_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>129.510000</td>\n",
       "      <td>187.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47.082239</td>\n",
       "      <td>73.808293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>53.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>102.000000</td>\n",
       "      <td>130.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>121.000000</td>\n",
       "      <td>178.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>151.250000</td>\n",
       "      <td>230.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>262.000000</td>\n",
       "      <td>410.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       null_completion_tokens  alt_completion_tokens\n",
       "count              100.000000             100.000000\n",
       "mean               129.510000             187.650000\n",
       "std                 47.082239              73.808293\n",
       "min                 51.000000              53.000000\n",
       "25%                102.000000             130.500000\n",
       "50%                121.000000             178.500000\n",
       "75%                151.250000             230.750000\n",
       "max                262.000000             410.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"null_completion_tokens\", \"alt_completion_tokens\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div style=\"font-size:18px\">The COT prompt achieved an F1 score of 0.76 on the 100 ABA ethics questions tested. In comparison, the null prompt had an F1 score of 0.77. Responses to the COT prompt contained an average of 187.65 tokens, took an average of 6.37 seconds to generate, and required max of 410 tokens. The null prompt received responses with an average of 129.51 tokens, took an average of 4.09 seconds to generate, and required max of 262 tokens.<br><br>The McNemar's test p-value was <strong>1.0</strong>. Which is <strong>not</strong> signficant.\n",
       "\n",
       " Prompt Myth Status: **Busted**</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Generate analysis\n",
    "if (results.f1.values[0] > results.f1.values[1]):\n",
    "    if (result.pvalue < 0.05):\n",
    "        prompt_myth_status = 'Confirmed'\n",
    "    if (result.pvalue < 0.50):\n",
    "        prompt_myth_status = 'Plausible'\n",
    "else:\n",
    "    prompt_myth_status = 'Busted'\n",
    "    \n",
    "\n",
    "font_size  = 18\n",
    "analysis = f\"\"\"<div style=\"font-size:{font_size}px\">The COT prompt achieved an F1 score of {round(results.f1.values[0], 2)} on the 100 ABA ethics questions tested. In comparison, the null prompt had an F1 score of {round(results.f1.values[1], 2)}. Responses to the COT prompt contained an average of {round(results.completion_tokens.values[0], 2)} tokens, took an average of {round(results.latency.values[0], 2)} seconds to generate, and required max of {round(df.alt_completion_tokens.max(), 2)} tokens. The null prompt received responses with an average of {round(results.completion_tokens.values[1], 2)} tokens, took an average of {round(results.latency.values[1], 2)} seconds to generate, and required max of {round(df.null_completion_tokens.max(), 2)} tokens.<br><br>The McNemar's test p-value was <strong>{round(result.pvalue, 2)}</strong>. Which is <strong>{sig}</strong> signficant.\"\"\"\n",
    "analysis += f'\\n\\n Prompt Myth Status: **{prompt_myth_status}**</div>'\n",
    "display(Markdown(analysis))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Doing multiple inferences will cost more and take more time; however, there is a significant advantage in guaranteeing a generation if for example the model reaches its max generation token limit before actually answering the question."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
