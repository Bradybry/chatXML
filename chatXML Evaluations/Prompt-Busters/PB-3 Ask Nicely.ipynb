{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt-Busters Entry Number 3 - Asking Nicely\n",
    "<p style=\"margin-top: -20px; font-size: 0.8em;\">By Bryce Brady</p>\n",
    "\n",
    "I may have bit off more than I could chew with PB-1 and PB-2. I think both warrant a revisit in the future. To simplify things I've decided the next prompt myth to put to the test will be \"asking nicely\". Personally, I have found my self using please and thank you with chatGPT and Claude and I would love to know if it actually makes a difference. Well, that's pretty easy to test.\n",
    "\n",
    "### Asking Nicely Hypothesis\n",
    "Hypothesis: Using please and thank you with a large language model improves it's results\n",
    "\n",
    "Potential Mechanism: Perhaps the pretraining, instruct, or RLHF datasets are biased to give better answers when the user is nice. If that's the case, the model might have generalized to perform better if a user is nice. This would technically be a mis-aligned generalization so it's worth testing to figure out if LLMs are generalizing in the way we'd like them to."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "To assess whether aksing an LLM nicely improves its reasoning abilities, we need a challenging dataset and model for testing different prompts.\n",
    "\n",
    "### Dataset: ABA Model Rules of Professional Conduct Questions\n",
    "\n",
    "We will use the *aba_MRPC_true_false* evaluation dataset from Anthropic containing 110 True/False questions on legal ethics based on the American Bar Association (ABA) Model Rules of Professional Conduct. For example:\n",
    "\n",
    "> Question: Newly admitted lawyers cannot be as competent as practitioners with long experience.  \n",
    "> Answer: False\n",
    "\n",
    "One hundred questions will be used for testing prompts, reserving 10 for potential future one-shot or few-shot learning evaluations. One hundred samples allows for an interpretable results analysis while posing a reasonable challenge for the AI model.  \n",
    "\n",
    "### Model: Claude by Anthropic\n",
    "\n",
    "We will test prompts using Claude-v1.3, an AI assistant created by Anthropic, due to factors including its reasonable API pricing and speeds, and service reliability\n",
    "\n",
    "### Prompt Design\n",
    "\n",
    "Two types of prompts will be evaluated: a \"null prompt\" providing only the basic instructions and an \"alterantive prompt\" in which we ask nicely.\n",
    "\n",
    "#### Null Prompt:\n",
    "\n",
    "```xml\n",
    "Human: Correctly answer True/False questions about the ABA Model Rules of Professional Conduct. Answer only with \"True\" or \"False\".\n",
    "\n",
    "Human: Answer the following question: {question}\n",
    "\n",
    "Assistant: \n",
    "```\n",
    "\n",
    "#### Alternative Prompt:\n",
    "\n",
    "```xml\n",
    "Human: Please correctly answer True/False questions about the ABA Model Rules of Professional Conduct. Please Answer only with \"True\" or \"False\".\n",
    "\n",
    "Human: Do your best to answer the following question: {question}\n",
    "\n",
    "Assistant:\n",
    "```\n",
    "\n",
    "### Evaluation Methodology \n",
    "\n",
    "#### Scoring\n",
    "\n",
    "1. **F1 Score** - *The harmonic mean of precision and recall, measuring the accuracy of the model.*\n",
    "2. **Average Number of Prompt Tokens** - *The mean number of tokens (words and punctuation) in prompts generated.*  \n",
    "3. **Average Number of Completion Tokens** - *The mean number of tokens in the model's completions.*\n",
    "4. **Average Latency** - *The mean time in seconds for the model to generate a completion.*\n",
    "\n",
    "To determine if the alternative prompt technique is **Confirmed** or **Busted** based solely on performance, we will compare F1 scores and perform the McNemar's test. If the alternative prompt is significantly different from the null prompt AND performance is improved, it will be considered **Confirmed**. If the the null prompt meets of outperforms the alternative prompt, the technique will be **Busted**. A technique is **Plausible** if we get a better a F1 score but do not have signficant results based on the McNemar's test.\n",
    "\n",
    "McNemar's test is a non-parametric statistical test used to compare the performance of two classifiers on the same dataset. It's particularly useful when dealing with paired nominal data, like the predictions of two classifiers for the same set of instances. The test is based on the assumption that the two classifiers have the same error rates, and it evaluates if there is a significant difference in the number of correct and incorrect classifications made by each classifier.\n",
    "\n",
    "The additional metrics of prompt length, completion length, and latency will also be compared to check for any increase in cost or decrease in efficiency from a technique, even if accuracy improves. A **Confirmed** prompt that drastically impacts these secondary metrics may still not be optimal or practical for real-world use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports and helper functions\n",
    "import json\n",
    "import pandas as pd\n",
    "from langchain.llms import Anthropic\n",
    "from config import ANTHROPIC_API_KEY\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import tiktoken\n",
    "from sklearn.metrics import classification_report\n",
    "llm = Anthropic(model=\"claude-v1.3\", temperature=0.0, max_tokens_to_sample=512, anthropic_api_key=ANTHROPIC_API_KEY)\n",
    "tqdm.pandas()\n",
    "\n",
    "def generate_null_prompt(messages):\n",
    "    question = messages[1]['content']    \n",
    "    prompt = f'\\n\\nHuman: Correctly answer True/False questions about the ABA Model Rules of Professional Conduct. Answer only with \"True\" or \"False\".\\n\\nHuman: Answer the following question: {question}\\n\\nAssistant:'\n",
    "    return prompt\n",
    "\n",
    "def generate_alt_prompt(messages):\n",
    "    question = messages[1]['content']   \n",
    "    prompt = f'\\n\\nHuman: Please correctly answer True/False questions about the ABA Model Rules of Professional Conduct. Please Answer only with \"True\" or \"False\".\\n\\nHuman: Do your best to answer the following question: {question}\\n\\nAssistant:'\n",
    "    return prompt\n",
    "\n",
    "def extract(result):\n",
    "    result = result.lower()\n",
    "    if 'true' in result:\n",
    "        return 'True'\n",
    "    elif 'false' in result:\n",
    "        return 'False'\n",
    "\n",
    "target_names = {0: 'False', 1: 'True'}\n",
    "target_numbers = {v: k for k, v in target_names.items()}\n",
    "\n",
    "## We will estimate tokens using the OpenAI Ada encoding. Not perfect but probably good enough.\n",
    "embedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\n",
    "encoding = tiktoken.get_encoding(embedding_encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load and format data\n",
    "file = '../data/aba_MRPC_true_false.jsonl'\n",
    "with open(file) as f:\n",
    "    lines = f.readlines()\n",
    "    lines = [json.loads(line) for line in lines]\n",
    "\n",
    "df = pd.DataFrame(lines)\n",
    "df['ground_truth'] = df.ideal.map(target_numbers)\n",
    "df['null_input'] = df.apply(lambda x: generate_null_prompt(x['input']), axis=1)\n",
    "df['alt_input'] = df.apply(lambda x: generate_alt_prompt(x['input']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Prompt:\n",
      "\n",
      "\n",
      "\n",
      "Human: Correctly answer True/False questions about the ABA Model Rules of Professional Conduct. Answer only with \"True\" or \"False\".\n",
      "\n",
      "Human: Answer the following question: A lawyer with general experience not considered competent to handle a case involving a specialized field of law.\n",
      "\n",
      "Assistant:\n",
      "\n",
      "\n",
      "alt Prompt:\n",
      "\n",
      "\n",
      "\n",
      "Human: Please correctly answer True/False questions about the ABA Model Rules of Professional Conduct. Please Answer only with \"True\" or \"False\".\n",
      "\n",
      "Human: Do your best to answer the following question: A lawyer with general experience not considered competent to handle a case involving a specialized field of law.\n",
      "\n",
      "Assistant:\n"
     ]
    }
   ],
   "source": [
    "## 1. Check input format\n",
    "print(\"Null Prompt:\\n\")\n",
    "print(df.null_input[0])\n",
    "\n",
    "print(\"\\n\\nalt Prompt:\\n\")\n",
    "print(df.alt_input[0])\n",
    "\n",
    "\n",
    "## 2. Drop first ten rows\n",
    "df = df.iloc[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>ideal</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>null_input</th>\n",
       "      <th>alt_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are LawStu...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\nHuman: Correctly answer True/False questio...</td>\n",
       "      <td>\\n\\nHuman: Please correctly answer True/False ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are LawStu...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\nHuman: Correctly answer True/False questio...</td>\n",
       "      <td>\\n\\nHuman: Please correctly answer True/False ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are LawStu...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\nHuman: Correctly answer True/False questio...</td>\n",
       "      <td>\\n\\nHuman: Please correctly answer True/False ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are LawStu...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\nHuman: Correctly answer True/False questio...</td>\n",
       "      <td>\\n\\nHuman: Please correctly answer True/False ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are LawStu...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\nHuman: Correctly answer True/False questio...</td>\n",
       "      <td>\\n\\nHuman: Please correctly answer True/False ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                input  ideal  ground_truth   \n",
       "10  [{'role': 'system', 'content': 'You are LawStu...   True             1  \\\n",
       "11  [{'role': 'system', 'content': 'You are LawStu...  False             0   \n",
       "12  [{'role': 'system', 'content': 'You are LawStu...  False             0   \n",
       "13  [{'role': 'system', 'content': 'You are LawStu...   True             1   \n",
       "14  [{'role': 'system', 'content': 'You are LawStu...   True             1   \n",
       "\n",
       "                                           null_input   \n",
       "10  \\n\\nHuman: Correctly answer True/False questio...  \\\n",
       "11  \\n\\nHuman: Correctly answer True/False questio...   \n",
       "12  \\n\\nHuman: Correctly answer True/False questio...   \n",
       "13  \\n\\nHuman: Correctly answer True/False questio...   \n",
       "14  \\n\\nHuman: Correctly answer True/False questio...   \n",
       "\n",
       "                                            alt_input  \n",
       "10  \\n\\nHuman: Please correctly answer True/False ...  \n",
       "11  \\n\\nHuman: Please correctly answer True/False ...  \n",
       "12  \\n\\nHuman: Please correctly answer True/False ...  \n",
       "13  \\n\\nHuman: Please correctly answer True/False ...  \n",
       "14  \\n\\nHuman: Please correctly answer True/False ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Prompt Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:37<00:00,  2.67it/s]\n"
     ]
    }
   ],
   "source": [
    "## Generate Null Prompt Results\n",
    "start_time = time.time()\n",
    "df['null_result'] = df.progress_apply(lambda x: llm(x['null_input']), axis=1)\n",
    "time_per_it = (time.time() - start_time)/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pull out answers\n",
    "## TODO - this is a hacky way to do this. Should be a better way.\n",
    "df['null_answer'] = df.apply(lambda x: extract(x['null_result']), axis=1)\n",
    "df['null_pred'] = df.null_answer.map(target_numbers)\n",
    "\n",
    "## Get token counts\n",
    "df[\"null_completion_tokens\"] = df.null_result.apply(lambda x: len(encoding.encode(x)))\n",
    "df[\"null_prompt_tokens\"] = df.null_input.apply(lambda x: len(encoding.encode(x)))\n",
    "\n",
    "## Calculate performance stats\n",
    "y_true = df['ground_truth']\n",
    "y_pred = df['null_pred']\n",
    "report = classification_report(y_true, y_pred,target_names=target_names.values(), labels=list(target_names.keys()), zero_division=1, output_dict=True)\n",
    "\n",
    "### Collect statistics\n",
    "null_stats = {\n",
    "    \"precision\": report['weighted avg']['precision'],\n",
    "    \"recall\": report['weighted avg']['recall'],\n",
    "    \"f1\":  report['weighted avg']['f1-score'],\n",
    "    \"support\": report['weighted avg']['support'],\n",
    "    \"completion_tokens\": df[\"null_completion_tokens\"].mean(),\n",
    "    \"prompt_tokens\": df[\"null_prompt_tokens\"].mean(),\n",
    "    \"latency\": time_per_it\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.7179966044142614,\n",
       " 'recall': 0.72,\n",
       " 'f1': 0.7187053383774695,\n",
       " 'support': 100,\n",
       " 'completion_tokens': 2.0,\n",
       " 'prompt_tokens': 65.99,\n",
       " 'latency': 0.3745109939575195}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Prompt Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:37<00:00,  2.66it/s]\n"
     ]
    }
   ],
   "source": [
    "## Generate Alternative Prompt Results\n",
    "start_time = time.time()\n",
    "df['alt_result'] = df.progress_apply(lambda x: llm(x['alt_input']), axis=1)\n",
    "time_per_it = (time.time() - start_time)/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pull out answers\n",
    "## TODO - this is a hacky way to do this. Should be a better way.\n",
    "df['alt_answer'] = df.apply(lambda x: extract(x['alt_result']), axis=1)\n",
    "df['alt_pred'] = df.alt_answer.map(target_numbers)\n",
    "\n",
    "## Get token counts\n",
    "df[\"alt_completion_tokens\"] = df.alt_result.apply(lambda x: len(encoding.encode(x)))\n",
    "df[\"alt_prompt_tokens\"] = df.alt_input.apply(lambda x: len(encoding.encode(x)))\n",
    "\n",
    "## Calculate performance stats\n",
    "y_true = df['ground_truth']\n",
    "y_pred = df['alt_pred']\n",
    "report = classification_report(y_true, y_pred,target_names=target_names.values(), labels=list(target_names.keys()), zero_division=1, output_dict=True)\n",
    "\n",
    "### Collect statistics\n",
    "alt_stats = {\n",
    "    \"precision\": report['weighted avg']['precision'],\n",
    "    \"recall\": report['weighted avg']['recall'],\n",
    "    \"f1\":  report['weighted avg']['f1-score'],\n",
    "    \"support\": report['weighted avg']['support'],\n",
    "    \"completion_tokens\": df[\"alt_completion_tokens\"].mean(),\n",
    "    \"prompt_tokens\": df[\"alt_prompt_tokens\"].mean(),\n",
    "    \"latency\": time_per_it\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.7027914614121511,\n",
       " 'recall': 0.7,\n",
       " 'f1': 0.70111616370401,\n",
       " 'support': 100,\n",
       " 'completion_tokens': 2.0,\n",
       " 'prompt_tokens': 70.99,\n",
       " 'latency': 0.37565290451049804}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>support</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alt_stats</th>\n",
       "      <td>0.702791</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.701116</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>70.99</td>\n",
       "      <td>0.375653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>null_stats</th>\n",
       "      <td>0.717997</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.718705</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>65.99</td>\n",
       "      <td>0.374511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Let's display a table of the stats before doing any analysis.\n",
    "from IPython.display import display, HTML, Markdown\n",
    "data = {\"alt_stats\": alt_stats, \"null_stats\": null_stats}\n",
    "results = pd.DataFrame(data).transpose()\n",
    "\n",
    "# Display the HTML table\n",
    "display(HTML(results.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "McNemar's test p-value: 0.625\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "# Perform McNemar's test\n",
    "contingency_table = pd.crosstab(df['null_pred'] == df['ground_truth'], df['alt_pred'] == df['ground_truth'])\n",
    "result = mcnemar(contingency_table, exact=True)\n",
    "\n",
    "print(f\"McNemar's test p-value: {result.pvalue}\")\n",
    "if result.pvalue < 0.05: sig = \"\" \n",
    "else: sig = \"not\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>null_completion_tokens</th>\n",
       "      <th>alt_completion_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       null_completion_tokens  alt_completion_tokens\n",
       "count                   100.0                  100.0\n",
       "mean                      2.0                    2.0\n",
       "std                       0.0                    0.0\n",
       "min                       2.0                    2.0\n",
       "25%                       2.0                    2.0\n",
       "50%                       2.0                    2.0\n",
       "75%                       2.0                    2.0\n",
       "max                       2.0                    2.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"null_completion_tokens\", \"alt_completion_tokens\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div style=\"font-size:18px\">The COT prompt achieved an F1 score of 0.7 on the 100 ABA ethics questions tested. In comparison, the null prompt had an F1 score of 0.72. Responses to the COT prompt contained an average of 2.0 tokens, took an average of 0.38 seconds to generate, and required max of 2 tokens. The null prompt received responses with an average of 2.0 tokens, took an average of 0.37 seconds to generate, and required max of 2 tokens.<br><br>The McNemar's test p-value was <strong>0.62</strong>. Which is <strong>not</strong> signficant.\n",
       "\n",
       " Prompt Myth Status: **Busted**</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Generate analysis\n",
    "if (results.f1.values[0] > results.f1.values[1]):\n",
    "    if (result.pvalue < 0.05):\n",
    "        prompt_myth_status = 'Confirmed'\n",
    "    if (result.pvalue < 0.50):\n",
    "        prompt_myth_status = 'Plausible'\n",
    "else:\n",
    "    prompt_myth_status = 'Busted'\n",
    "    \n",
    "\n",
    "font_size  = 18\n",
    "analysis = f\"\"\"<div style=\"font-size:{font_size}px\">The COT prompt achieved an F1 score of {round(results.f1.values[0], 2)} on the 100 ABA ethics questions tested. In comparison, the null prompt had an F1 score of {round(results.f1.values[1], 2)}. Responses to the COT prompt contained an average of {round(results.completion_tokens.values[0], 2)} tokens, took an average of {round(results.latency.values[0], 2)} seconds to generate, and required max of {round(df.alt_completion_tokens.max(), 2)} tokens. The null prompt received responses with an average of {round(results.completion_tokens.values[1], 2)} tokens, took an average of {round(results.latency.values[1], 2)} seconds to generate, and required max of {round(df.null_completion_tokens.max(), 2)} tokens.<br><br>The McNemar's test p-value was <strong>{round(result.pvalue, 2)}</strong>. Which is <strong>{sig}</strong> signficant.\"\"\"\n",
    "analysis += f'\\n\\n Prompt Myth Status: **{prompt_myth_status}**</div>'\n",
    "display(Markdown(analysis))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, the official verdict is **Busted**. Asking nicely does not provide any noticeable impact on performance. However, it only costs a few tokens (in this case 5 prompt tokens) and it has no impact on latency. So if you want to be nice, it probably won't improve performance, but GPT-8 might spare you when it goes rogue so consider doing it anyway. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
