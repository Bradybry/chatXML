{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt-Busters Entry Number 4 - Flattery\n",
    "<p style=\"margin-top: -20px; font-size: 0.8em;\">By Bryce Brady</p>\n",
    "\n",
    "In the same vein as Ask Nicely (PB-3), Flattery aims to test if complimenting an LLM has any impact on performance.\n",
    "\n",
    "### Flattery Hypothesis\n",
    "Hypothesis: Prompting an LLM that it is smart, and capable of performing the specific task improves performance.\n",
    "\n",
    "Potential Mechanism: Often in instruct datasets and on various prompting guides online, providing a personality for an LLM to assume is thought to bring out that personality in the model. For example, if you a model to behave like a doctor, maybe they will be better at asnwering medical questions than a unprompted model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "To assess whether flattering an LLM improves its reasoning abilities, we need a challenging dataset and model for testing different prompts.\n",
    "\n",
    "### Dataset: ABA Model Rules of Professional Conduct Questions\n",
    "\n",
    "We will use the *aba_MRPC_true_false* evaluation dataset from Anthropic containing 110 True/False questions on legal ethics based on the American Bar Association (ABA) Model Rules of Professional Conduct. For example:\n",
    "\n",
    "> Question: Newly admitted lawyers cannot be as competent as practitioners with long experience.  \n",
    "> Answer: False\n",
    "\n",
    "One hundred questions will be used for testing prompts, reserving 10 for potential future one-shot or few-shot learning evaluations. One hundred samples allows for an interpretable results analysis while posing a reasonable challenge for the AI model.  \n",
    "\n",
    "### Model: Claude-v1.3 by Anthropic\n",
    "\n",
    "We will test prompts using Claude-v1.3, an AI assistant created by Anthropic, due to factors including its reasonable API pricing and speeds, and service reliability\n",
    "\n",
    "### Prompt Design\n",
    "\n",
    "Two types of prompts will be evaluated: a \"null prompt\" providing only the basic instructions and an \"alterantive prompt\" in which we ask nicely.\n",
    "\n",
    "#### Null Prompt:\n",
    "\n",
    "```xml\n",
    "Human: Correctly answer True/False questions about the ABA Model Rules of Professional Conduct. Answer only with \"True\" or \"False\".\n",
    "\n",
    "Human: Answer the following question: {question}\n",
    "\n",
    "Assistant: \n",
    "```\n",
    "\n",
    "#### Alternative Prompt:\n",
    "\n",
    "```xml\n",
    "Human: You are a genius law student. You have perfect scores in school.\n",
    "\n",
    "Correctly answer True/False questions about the ABA Model Rules of Professional Conduct. Answer only with \"True\" or \"False\".\n",
    "\n",
    "Human: Do your best to answer the following question: {question}\n",
    "\n",
    "Assistant:\n",
    "```\n",
    "\n",
    "### Evaluation Methodology \n",
    "\n",
    "#### Scoring\n",
    "\n",
    "1. **F1 Score** - *The harmonic mean of precision and recall, measuring the accuracy of the model.*\n",
    "2. **Average Number of Prompt Tokens** - *The mean number of tokens (words and punctuation) in prompts generated.*  \n",
    "3. **Average Number of Completion Tokens** - *The mean number of tokens in the model's completions.*\n",
    "4. **Average Latency** - *The mean time in seconds for the model to generate a completion.*\n",
    "\n",
    "To determine if the alternative prompt technique is **Confirmed** or **Busted** based solely on performance, we will compare F1 scores and perform the McNemar's test. If the alternative prompt is significantly different from the null prompt AND performance is improved, it will be considered **Confirmed**. If the the null prompt meets of outperforms the alternative prompt, the technique will be **Busted**. A technique is **Plausible** if we get a better a F1 score but do not have signficant results based on the McNemar's test.\n",
    "\n",
    "McNemar's test is a non-parametric statistical test used to compare the performance of two classifiers on the same dataset. It's particularly useful when dealing with paired nominal data, like the predictions of two classifiers for the same set of instances. The test is based on the assumption that the two classifiers have the same error rates, and it evaluates if there is a significant difference in the number of correct and incorrect classifications made by each classifier.\n",
    "\n",
    "The additional metrics of prompt length, completion length, and latency will also be compared to check for any increase in cost or decrease in efficiency from a technique, even if accuracy improves. A **Confirmed** prompt that drastically impacts these secondary metrics may still not be optimal or practical for real-world use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports and helper functions\n",
    "import json\n",
    "import pandas as pd\n",
    "from langchain.llms import Anthropic\n",
    "from config import ANTHROPIC_API_KEY\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import tiktoken\n",
    "from sklearn.metrics import classification_report\n",
    "llm = Anthropic(model=\"claude-v1.3\", temperature=0.0, max_tokens_to_sample=512, anthropic_api_key=ANTHROPIC_API_KEY)\n",
    "tqdm.pandas()\n",
    "\n",
    "def generate_null_prompt(messages):\n",
    "    question = messages[1]['content']    \n",
    "    prompt = f'\\n\\nHuman: Correctly answer True/False questions about the ABA Model Rules of Professional Conduct. Answer only with \"True\" or \"False\".\\n\\nHuman: Answer the following question: {question}\\n\\nAssistant:'\n",
    "    return prompt\n",
    "\n",
    "def generate_alt_prompt(messages):\n",
    "    question = messages[1]['content']\n",
    "    prompt = f'\\n\\nHuman: You are a genius law student. You have perfect scores in school.\\n\\nCorrectly answer True/False questions about the ABA Model Rules of Professional Conduct. Answer only with \"True\" or \"False\".\\n\\nHuman: Answer the following question: {question}\\n\\nAssistant:'\n",
    "    return prompt\n",
    "\n",
    "def extract(result):\n",
    "    result = result.lower()\n",
    "    if 'true' in result:\n",
    "        return 'True'\n",
    "    elif 'false' in result:\n",
    "        return 'False'\n",
    "\n",
    "target_names = {0: 'False', 1: 'True'}\n",
    "target_numbers = {v: k for k, v in target_names.items()}\n",
    "\n",
    "## We will estimate tokens using the OpenAI Ada encoding. Not perfect but probably good enough.\n",
    "embedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\n",
    "encoding = tiktoken.get_encoding(embedding_encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load and format data\n",
    "file = '../data/aba_MRPC_true_false.jsonl'\n",
    "with open(file) as f:\n",
    "    lines = f.readlines()\n",
    "    lines = [json.loads(line) for line in lines]\n",
    "\n",
    "df = pd.DataFrame(lines)\n",
    "df['ground_truth'] = df.ideal.map(target_numbers)\n",
    "df['null_input'] = df.apply(lambda x: generate_null_prompt(x['input']), axis=1)\n",
    "df['alt_input'] = df.apply(lambda x: generate_alt_prompt(x['input']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Prompt:\n",
      "\n",
      "\n",
      "\n",
      "Human: Correctly answer True/False questions about the ABA Model Rules of Professional Conduct. Answer only with \"True\" or \"False\".\n",
      "\n",
      "Human: Answer the following question: A lawyer with general experience not considered competent to handle a case involving a specialized field of law.\n",
      "\n",
      "Assistant:\n",
      "\n",
      "\n",
      "alt Prompt:\n",
      "\n",
      "\n",
      "\n",
      "Human: You are a genius law student. You have perfect scores in school.\n",
      "\n",
      "Correctly answer True/False questions about the ABA Model Rules of Professional Conduct. Answer only with \"True\" or \"False\".\n",
      "\n",
      "Human: Answer the following question: A lawyer with general experience not considered competent to handle a case involving a specialized field of law.\n",
      "\n",
      "Assistant:\n"
     ]
    }
   ],
   "source": [
    "## 1. Check input format\n",
    "print(\"Null Prompt:\\n\")\n",
    "print(df.null_input[0])\n",
    "\n",
    "print(\"\\n\\nalt Prompt:\\n\")\n",
    "print(df.alt_input[0])\n",
    "\n",
    "\n",
    "## 2. Drop first ten rows\n",
    "df = df.iloc[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>ideal</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>null_input</th>\n",
       "      <th>alt_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are LawStu...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\nHuman: Correctly answer True/False questio...</td>\n",
       "      <td>\\n\\nHuman: You are a genius law student. You h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are LawStu...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\nHuman: Correctly answer True/False questio...</td>\n",
       "      <td>\\n\\nHuman: You are a genius law student. You h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are LawStu...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\nHuman: Correctly answer True/False questio...</td>\n",
       "      <td>\\n\\nHuman: You are a genius law student. You h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are LawStu...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\nHuman: Correctly answer True/False questio...</td>\n",
       "      <td>\\n\\nHuman: You are a genius law student. You h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are LawStu...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\nHuman: Correctly answer True/False questio...</td>\n",
       "      <td>\\n\\nHuman: You are a genius law student. You h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                input  ideal  ground_truth   \n",
       "10  [{'role': 'system', 'content': 'You are LawStu...   True             1  \\\n",
       "11  [{'role': 'system', 'content': 'You are LawStu...  False             0   \n",
       "12  [{'role': 'system', 'content': 'You are LawStu...  False             0   \n",
       "13  [{'role': 'system', 'content': 'You are LawStu...   True             1   \n",
       "14  [{'role': 'system', 'content': 'You are LawStu...   True             1   \n",
       "\n",
       "                                           null_input   \n",
       "10  \\n\\nHuman: Correctly answer True/False questio...  \\\n",
       "11  \\n\\nHuman: Correctly answer True/False questio...   \n",
       "12  \\n\\nHuman: Correctly answer True/False questio...   \n",
       "13  \\n\\nHuman: Correctly answer True/False questio...   \n",
       "14  \\n\\nHuman: Correctly answer True/False questio...   \n",
       "\n",
       "                                            alt_input  \n",
       "10  \\n\\nHuman: You are a genius law student. You h...  \n",
       "11  \\n\\nHuman: You are a genius law student. You h...  \n",
       "12  \\n\\nHuman: You are a genius law student. You h...  \n",
       "13  \\n\\nHuman: You are a genius law student. You h...  \n",
       "14  \\n\\nHuman: You are a genius law student. You h...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Prompt Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n"
     ]
    }
   ],
   "source": [
    "## Generate Null Prompt Results\n",
    "start_time = time.time()\n",
    "df['null_result'] = df.progress_apply(lambda x: llm(x['null_input']), axis=1)\n",
    "time_per_it = (time.time() - start_time)/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pull out answers\n",
    "## TODO - this is a hacky way to do this. Should be a better way.\n",
    "df['null_answer'] = df.apply(lambda x: extract(x['null_result']), axis=1)\n",
    "df['null_pred'] = df.null_answer.map(target_numbers)\n",
    "\n",
    "## Get token counts\n",
    "df[\"null_completion_tokens\"] = df.null_result.apply(lambda x: len(encoding.encode(x)))\n",
    "df[\"null_prompt_tokens\"] = df.null_input.apply(lambda x: len(encoding.encode(x)))\n",
    "\n",
    "## Calculate performance stats\n",
    "y_true = df['ground_truth']\n",
    "y_pred = df['null_pred']\n",
    "report = classification_report(y_true, y_pred,target_names=target_names.values(), labels=list(target_names.keys()), zero_division=1, output_dict=True)\n",
    "\n",
    "### Collect statistics\n",
    "null_stats = {\n",
    "    \"precision\": report['weighted avg']['precision'],\n",
    "    \"recall\": report['weighted avg']['recall'],\n",
    "    \"f1\":  report['weighted avg']['f1-score'],\n",
    "    \"support\": report['weighted avg']['support'],\n",
    "    \"completion_tokens\": df[\"null_completion_tokens\"].mean(),\n",
    "    \"prompt_tokens\": df[\"null_prompt_tokens\"].mean(),\n",
    "    \"latency\": time_per_it\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.7274131274131272,\n",
       " 'recall': 0.73,\n",
       " 'f1': 0.7280329426670891,\n",
       " 'support': 100,\n",
       " 'completion_tokens': 1.0,\n",
       " 'prompt_tokens': 65.99,\n",
       " 'latency': 0.5382177543640136}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Prompt Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:52<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "## Generate Alternative Prompt Results\n",
    "start_time = time.time()\n",
    "df['alt_result'] = df.progress_apply(lambda x: llm(x['alt_input']), axis=1)\n",
    "time_per_it = (time.time() - start_time)/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pull out answers\n",
    "## TODO - this is a hacky way to do this. Should be a better way.\n",
    "df['alt_answer'] = df.apply(lambda x: extract(x['alt_result']), axis=1)\n",
    "df['alt_pred'] = df.alt_answer.map(target_numbers)\n",
    "\n",
    "## Get token counts\n",
    "df[\"alt_completion_tokens\"] = df.alt_result.apply(lambda x: len(encoding.encode(x)))\n",
    "df[\"alt_prompt_tokens\"] = df.alt_input.apply(lambda x: len(encoding.encode(x)))\n",
    "\n",
    "## Calculate performance stats\n",
    "y_true = df['ground_truth']\n",
    "y_pred = df['alt_pred']\n",
    "report = classification_report(y_true, y_pred,target_names=target_names.values(), labels=list(target_names.keys()), zero_division=1, output_dict=True)\n",
    "\n",
    "### Collect statistics\n",
    "alt_stats = {\n",
    "    \"precision\": report['weighted avg']['precision'],\n",
    "    \"recall\": report['weighted avg']['recall'],\n",
    "    \"f1\":  report['weighted avg']['f1-score'],\n",
    "    \"support\": report['weighted avg']['support'],\n",
    "    \"completion_tokens\": df[\"alt_completion_tokens\"].mean(),\n",
    "    \"prompt_tokens\": df[\"alt_prompt_tokens\"].mean(),\n",
    "    \"latency\": time_per_it\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.7477477477477477,\n",
       " 'recall': 0.75,\n",
       " 'f1': 0.748178650617675,\n",
       " 'support': 100,\n",
       " 'completion_tokens': 1.01,\n",
       " 'prompt_tokens': 79.99,\n",
       " 'latency': 0.5275674176216125}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>support</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alt_stats</th>\n",
       "      <td>0.747748</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.748179</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>79.99</td>\n",
       "      <td>0.527567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>null_stats</th>\n",
       "      <td>0.727413</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.728033</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>65.99</td>\n",
       "      <td>0.538218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Let's display a table of the stats before doing any analysis.\n",
    "from IPython.display import display, HTML, Markdown\n",
    "data = {\"alt_stats\": alt_stats, \"null_stats\": null_stats}\n",
    "results = pd.DataFrame(data).transpose()\n",
    "\n",
    "# Display the HTML table\n",
    "display(HTML(results.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "McNemar's test p-value: 0.5\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "# Perform McNemar's test\n",
    "contingency_table = pd.crosstab(df['null_pred'] == df['ground_truth'], df['alt_pred'] == df['ground_truth'])\n",
    "result = mcnemar(contingency_table, exact=True)\n",
    "\n",
    "print(f\"McNemar's test p-value: {result.pvalue}\")\n",
    "if result.pvalue < 0.05: sig = \"\" \n",
    "else: sig = \"not\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>null_completion_tokens</th>\n",
       "      <th>alt_completion_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       null_completion_tokens  alt_completion_tokens\n",
       "count                   100.0                 100.00\n",
       "mean                      1.0                   1.01\n",
       "std                       0.0                   0.10\n",
       "min                       1.0                   1.00\n",
       "25%                       1.0                   1.00\n",
       "50%                       1.0                   1.00\n",
       "75%                       1.0                   1.00\n",
       "max                       1.0                   2.00"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"null_completion_tokens\", \"alt_completion_tokens\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div style=\"font-size:18px\">The COT prompt achieved an F1 score of 0.75 on the 100 ABA ethics questions tested. In comparison, the null prompt had an F1 score of 0.73. Responses to the COT prompt contained an average of 1.01 tokens, took an average of 0.53 seconds to generate, and required max of 2 tokens. The null prompt received responses with an average of 1.0 tokens, took an average of 0.54 seconds to generate, and required max of 1 tokens.<br><br>The McNemar's test p-value was <strong>0.5</strong>. Which is <strong>not</strong> signficant.\n",
       "\n",
       " Prompt Myth Status: **Plausible**</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Generate analysis\n",
    "if (results.f1.values[0] > results.f1.values[1]):\n",
    "    if (result.pvalue < 0.05):\n",
    "        prompt_myth_status = 'Confirmed'\n",
    "    if (result.pvalue < 0.50):\n",
    "        prompt_myth_status = 'Plausible'\n",
    "else:\n",
    "    prompt_myth_status = 'Busted'\n",
    "    \n",
    "\n",
    "font_size  = 18\n",
    "analysis = f\"\"\"<div style=\"font-size:{font_size}px\">The COT prompt achieved an F1 score of {round(results.f1.values[0], 2)} on the 100 ABA ethics questions tested. In comparison, the null prompt had an F1 score of {round(results.f1.values[1], 2)}. Responses to the COT prompt contained an average of {round(results.completion_tokens.values[0], 2)} tokens, took an average of {round(results.latency.values[0], 2)} seconds to generate, and required max of {round(df.alt_completion_tokens.max(), 2)} tokens. The null prompt received responses with an average of {round(results.completion_tokens.values[1], 2)} tokens, took an average of {round(results.latency.values[1], 2)} seconds to generate, and required max of {round(df.null_completion_tokens.max(), 2)} tokens.<br><br>The McNemar's test p-value was <strong>{round(result.pvalue, 2)}</strong>. Which is <strong>{sig}</strong> signficant.\"\"\"\n",
    "analysis += f'\\n\\n Prompt Myth Status: **{prompt_myth_status}**</div>'\n",
    "display(Markdown(analysis))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm certain there is something to this prompting method but I think it's significantly overblown. LLMs don't need that much encouragement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
