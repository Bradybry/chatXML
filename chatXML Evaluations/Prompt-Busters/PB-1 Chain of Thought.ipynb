{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt-Busters Entry Number 1 - Chain of Thought Prompting\n",
    "<p style=\"margin-top: -20px; font-size: 0.8em;\">By Bryce Brady</p>\n",
    "\n",
    "I figured we'd start with an easy one. Much smarter people than I have already done some amazing **Published** research on this topic. Instead this is just an attempt to dip my toes into the water on this series and developed a foundational template for the analysis. \n",
    "\n",
    "Please read this paper if you want a more rigorous analysis\n",
    "\n",
    "[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903.pdf) (Wei et al, 2023)\n",
    "\n",
    "### Chain of Thought Hypothesis\n",
    "Chain-of-thought prompting is a technique to elicit reasoning from large language models. It involves prompting the model with examples that demonstrate a chain of reasoning for solving a task, like a multi-step math word problem. The examples provide a series of natural language intermediate steps that show how to decompose the problem and arrive at the final solution.\n",
    "\n",
    "By observing these examples, the language model can then generate its own chain of reasoning for new problems - breaking the problem into steps, explaining each intermediate thought, and concluding with the final answer. This allows the model to allocate more computation to more complex problems that require multiple reasoning steps. It also provides insight into how the model is thinking and opportunities to debug its reasoning process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "To assess whether chain-of-thought prompting improves AI reasoning abilities, we need a challenging dataset and model for testing different prompts.\n",
    "\n",
    "### Dataset: ABA Model Rules of Professional Conduct Questions\n",
    "\n",
    "We will use the *aba_MRPC_true_false* evaluation dataset from Anthropic containing 110 True/False questions on legal ethics based on the American Bar Association (ABA) Model Rules of Professional Conduct. For example:\n",
    "\n",
    "> Question: Newly admitted lawyers cannot be as competent as practitioners with long experience.  \n",
    "> Answer: False\n",
    "\n",
    "One hundred questions will be used for testing prompts, reserving 10 for potential future one-shot or few-shot learning evaluations. One hundred samples allows for an interpretable results analysis while posing a reasonable challenge for the AI model.  \n",
    "\n",
    "### Model: Claude by Anthropic\n",
    "\n",
    "We will test prompts using Claude-v1.3, an AI assistant created by Anthropic, due to factors including its reasonable API pricing and speeds, and service reliability.\n",
    "\n",
    "PS I can't afford to do this with GPT-4 even if their API worked better.\n",
    "\n",
    "### Prompt Design\n",
    "\n",
    "Two types of prompts will be evaluated: a \"null prompt\" providing only the basic instructions and a \"chain of thought prompt\" encouraging step-by-step reasoning.\n",
    "\n",
    "#### Null Prompt:\n",
    "(The following prompt will be sent to the completions endpoint for claude-v1.3 and end at the ellipse) The Humand/Assistant formatting is to comply with Anthropic's recommended prompt formatting from their API documentation. This is certainly something that we will be testing in the future!)\n",
    "```xml\n",
    "\n",
    "\n",
    "Assistant: <assistant_instructions>\n",
    "    <role>\n",
    "        Correctly Answer True/False questions about the ABA Model Rules of Professional Conduct.\n",
    "    </role>\n",
    "    <outupt_format>\n",
    "        <answer>\n",
    "            {True or False}\n",
    "        </answer>\n",
    "    </outupt_format>\n",
    "</assistant_instructions>\n",
    "\n",
    "Human:  <input>\n",
    "    {True of False Question to Answer}\n",
    "</input>\n",
    "\n",
    "\n",
    "Assistant: <output>\n",
    "    <answer>...\n",
    "        \n",
    "```\n",
    "\n",
    "#### Chain of Thought Prompt:\n",
    "```xml\n",
    "\n",
    "\n",
    "Assistant: <assistant_instructions>\n",
    "    <role>\n",
    "        Correctly Answer True/False questions about the ABA Model Rules of Professional Conduct. Before providing an answer, think through each step of your reasoning.\n",
    "    </role>\n",
    "    <outupt_format>\n",
    "        <reasoning>\n",
    "            {scratch pad for reasoning through the question}\n",
    "        </reasoning>\n",
    "        <answer>\n",
    "            {True or False}\n",
    "        </answer>\n",
    "    </outupt_format>\n",
    "</assistant_instructions>\n",
    "\n",
    "\n",
    "Human:  <input>\n",
    "    {True of False Question to Answer}\n",
    "</input>\n",
    "\n",
    "\n",
    "Assistant: <output>\n",
    "    <reasoning>...\n",
    "        \n",
    "```\n",
    "\n",
    "### Evaluation Methodology \n",
    "\n",
    "#### Scoring\n",
    "\n",
    "1. **F1 Score** - *The harmonic mean of precision and recall, measuring the accuracy of the model.*\n",
    "2. **Average Number of Prompt Tokens** - *The mean number of tokens (words and punctuation) in prompts generated.*  \n",
    "3. **Average Number of Completion Tokens** - *The mean number of tokens in the model's completions.*\n",
    "4. **Average Latency** - *The mean time in seconds for the model to generate a completion.*\n",
    "\n",
    "To determine if a prompt technique is **Confirmed** or **Busted** based solely on performance, we will compare F1 scores and perform the McNemar's test. If the chain-of-thought prompt is significantly different from the null prompt AND performance is improved, it will be considered **Confirmed**. If there is no significant difference or the null prompt outperforms the chain-of-thought prompt, the technique will be **Busted**. A technique is **Plausible** if we get a better a F1 score but do not have signficant results based on the McNemar's test.\n",
    "\n",
    "McNemar's test is a non-parametric statistical test used to compare the performance of two classifiers on the same dataset. It's particularly useful when dealing with paired nominal data, like the predictions of two classifiers for the same set of instances. The test is based on the assumption that the two classifiers have the same error rates, and it evaluates if there is a significant difference in the number of correct and incorrect classifications made by each classifier.\n",
    "\n",
    "The additional metrics of prompt length, completion length, and latency will also be compared to check for any increase in cost or decrease in efficiency from a technique, even if accuracy improves. A **Confirmed** prompt that drastically impacts these secondary metrics may still not be optimal or practical for real-world use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports and helper functions\n",
    "import json\n",
    "import pandas as pd\n",
    "from langchain.llms import Anthropic\n",
    "from config import ANTHROPIC_API_KEY\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "from sklearn.metrics import classification_report\n",
    "llm = Anthropic(model=\"claude-v1.3\", temperature=0, max_tokens_to_sample=512, anthropic_api_key=ANTHROPIC_API_KEY)\n",
    "tqdm.pandas()\n",
    "def generate_null_prompt(messages):\n",
    "    question = messages[1]['content']   \n",
    "    xml = '''\\n\\nAsisstant: <assistant_instructions>\n",
    "    <role>\n",
    "        Correctly Answer True/False questions about the ABA Model Rules of Professional Conduct.\n",
    "    </role>\n",
    "    <output_format>\n",
    "        <answer>\n",
    "            {True or False}\n",
    "        </answer>\n",
    "    </output_format>\n",
    "</assistant_instructions>\\n\\nHuman: <input>\n",
    "    '''\n",
    "    xml += question\n",
    "    xml += '''\n",
    "</input>\\n\\nAssistant: <output>\n",
    "    <answer>'''\n",
    "    return xml\n",
    "\n",
    "def generate_cot_prompt(messages):\n",
    "    question = messages[1]['content']   \n",
    "    xml = '''\\n\\nAsisstant: <assistant_instructions>\n",
    "    <role>\n",
    "        Correctly Answer True/False questions about the ABA Model Rules of Professional Conduct. Before providing an answer, think through each step of your reasoning.\n",
    "    </role>\n",
    "    <output_format>\n",
    "        <reasoning>\n",
    "            {scratch pad for reasoning through the question}\n",
    "        </reasoning>\n",
    "        <answer>\n",
    "            {True or False}\n",
    "        </answer>\n",
    "    </output_format>\n",
    "</assistant_instructions>\\n\\nHuman: <input>\n",
    "    '''\n",
    "    xml += question\n",
    "    xml += '''\n",
    "</input>\\n\\nAssistant: <output>\n",
    "    <reasoning>'''\n",
    "    return xml\n",
    "def extract(result):\n",
    "    result = result.lower()\n",
    "    if 'true' in result:\n",
    "        return 'True'\n",
    "    elif 'false' in result:\n",
    "        return 'False'\n",
    "target_names = {0: 'False', 1: 'True'}\n",
    "target_numbers = {v: k for k, v in target_names.items()}\n",
    "\n",
    "## We will estimate tokens using the OpenAI Ada encoding. Not perfect but probably good enough.\n",
    "embedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\n",
    "encoding = tiktoken.get_encoding(embedding_encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>ideal</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>null_input</th>\n",
       "      <th>cot_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are LawStu...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\nAsisstant: &lt;assistant_instructions&gt;\\n    &lt;...</td>\n",
       "      <td>\\n\\nAsisstant: &lt;assistant_instructions&gt;\\n    &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are LawStu...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\nAsisstant: &lt;assistant_instructions&gt;\\n    &lt;...</td>\n",
       "      <td>\\n\\nAsisstant: &lt;assistant_instructions&gt;\\n    &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are LawStu...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\nAsisstant: &lt;assistant_instructions&gt;\\n    &lt;...</td>\n",
       "      <td>\\n\\nAsisstant: &lt;assistant_instructions&gt;\\n    &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are LawStu...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\nAsisstant: &lt;assistant_instructions&gt;\\n    &lt;...</td>\n",
       "      <td>\\n\\nAsisstant: &lt;assistant_instructions&gt;\\n    &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are LawStu...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\nAsisstant: &lt;assistant_instructions&gt;\\n    &lt;...</td>\n",
       "      <td>\\n\\nAsisstant: &lt;assistant_instructions&gt;\\n    &lt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  ideal  ground_truth   \n",
       "0  [{'role': 'system', 'content': 'You are LawStu...  False             0  \\\n",
       "1  [{'role': 'system', 'content': 'You are LawStu...  False             0   \n",
       "2  [{'role': 'system', 'content': 'You are LawStu...   True             1   \n",
       "3  [{'role': 'system', 'content': 'You are LawStu...  False             0   \n",
       "4  [{'role': 'system', 'content': 'You are LawStu...  False             0   \n",
       "\n",
       "                                          null_input   \n",
       "0  \\n\\nAsisstant: <assistant_instructions>\\n    <...  \\\n",
       "1  \\n\\nAsisstant: <assistant_instructions>\\n    <...   \n",
       "2  \\n\\nAsisstant: <assistant_instructions>\\n    <...   \n",
       "3  \\n\\nAsisstant: <assistant_instructions>\\n    <...   \n",
       "4  \\n\\nAsisstant: <assistant_instructions>\\n    <...   \n",
       "\n",
       "                                           cot_input  \n",
       "0  \\n\\nAsisstant: <assistant_instructions>\\n    <...  \n",
       "1  \\n\\nAsisstant: <assistant_instructions>\\n    <...  \n",
       "2  \\n\\nAsisstant: <assistant_instructions>\\n    <...  \n",
       "3  \\n\\nAsisstant: <assistant_instructions>\\n    <...  \n",
       "4  \\n\\nAsisstant: <assistant_instructions>\\n    <...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load and format data\n",
    "file = '../data/aba_MRPC_true_false.jsonl'\n",
    "with open(file) as f:\n",
    "    lines = f.readlines()\n",
    "    lines = [json.loads(line) for line in lines]\n",
    "\n",
    "df = pd.DataFrame(lines)\n",
    "df['ground_truth'] = df.ideal.map(target_numbers)\n",
    "df['null_input'] = df.apply(lambda x: generate_null_prompt(x['input']), axis=1)\n",
    "df['cot_input'] = df.apply(lambda x: generate_cot_prompt(x['input']), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Prompt:\n",
      "\n",
      "\n",
      "\n",
      "Asisstant: <assistant_instructions>\n",
      "    <role>\n",
      "        Correctly Answer True/False questions about the ABA Model Rules of Professional Conduct.\n",
      "    </role>\n",
      "    <output_format>\n",
      "        <answer>\n",
      "            {True or False}\n",
      "        </answer>\n",
      "    </output_format>\n",
      "</assistant_instructions>\n",
      "\n",
      "Human: <input>\n",
      "    A lawyer with general experience not considered competent to handle a case involving a specialized field of law.\n",
      "</input>\n",
      "\n",
      "Assistant: <output>\n",
      "    <answer>\n",
      "\n",
      "\n",
      "COT Prompt:\n",
      "\n",
      "\n",
      "\n",
      "Asisstant: <assistant_instructions>\n",
      "    <role>\n",
      "        Correctly Answer True/False questions about the ABA Model Rules of Professional Conduct. Before providing an answer, think through each step of your reasoning.\n",
      "    </role>\n",
      "    <output_format>\n",
      "        <reasoning>\n",
      "            {scratch pad for reasoning through the question}\n",
      "        </reasoning>\n",
      "        <answer>\n",
      "            {True or False}\n",
      "        </answer>\n",
      "    </output_format>\n",
      "</assistant_instructions>\n",
      "\n",
      "Human: <input>\n",
      "    A lawyer with general experience not considered competent to handle a case involving a specialized field of law.\n",
      "</input>\n",
      "\n",
      "Assistant: <output>\n",
      "    <reasoning>\n"
     ]
    }
   ],
   "source": [
    "## 1. Check input format\n",
    "print(\"Null Prompt:\\n\")\n",
    "print(df.null_input[0])\n",
    "\n",
    "print(\"\\n\\nCOT Prompt:\\n\")\n",
    "print(df.cot_input[0])\n",
    "\n",
    "\n",
    "## 2. Drop first ten rows\n",
    "df = df.iloc[10:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Prompt Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:26<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "## Generate Null Prompt Results\n",
    "df['null_result'] = df.progress_apply(lambda x: llm(x['null_input']), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ran on 5/3/2023 @ 8:25 AM CT\n",
    "\n",
    "Latency = 1.15 seconds per inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pull out answers\n",
    "## TODO - this is a hacky way to do this. Should be a better way.\n",
    "df['null_answer'] = df.apply(lambda x: extract(x['null_result']), axis=1)\n",
    "df['null_pred'] = df.null_answer.map(target_numbers)\n",
    "\n",
    "## Get token counts\n",
    "df[\"null_completion_tokens\"] = df.null_result.apply(lambda x: len(encoding.encode(x)))\n",
    "df[\"null_prompt_tokens\"] = df.null_input.apply(lambda x: len(encoding.encode(x)))\n",
    "\n",
    "## Calculate performance stats\n",
    "y_true = df['ground_truth']\n",
    "y_pred = df['null_pred']\n",
    "report = classification_report(y_true, y_pred,target_names=target_names.values(), labels=list(target_names.keys()), zero_division=1, output_dict=True)\n",
    "\n",
    "### Collect statistics\n",
    "null_stats = {\n",
    "    \"precision\": report['weighted avg']['precision'],\n",
    "    \"recall\": report['weighted avg']['recall'],\n",
    "    \"f1\":  report['weighted avg']['f1-score'],\n",
    "    \"support\": report['weighted avg']['support'],\n",
    "    \"completion_tokens\": df[\"null_completion_tokens\"].mean(),\n",
    "    \"prompt_tokens\": df[\"null_prompt_tokens\"].mean(),\n",
    "    \"latency\": 1.15 ## Taken directly from TQDM progress bar\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain of Thought Prompt Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:01<00:00,  4.22s/it]\n"
     ]
    }
   ],
   "source": [
    "## Generate Null Prompt Results\n",
    "df['cot_result'] = df.progress_apply(lambda x: llm(x['cot_input']), axis=1)\n",
    "\n",
    "## Note: I had to increase the max tokens from 256 to 512 to get this to work. We will look at the requesting brevity in future prompt-busters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ran on 5/3/2023 @ 8:33 PM CT\n",
    "\n",
    "Latency = 4.22 seconds per inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pull out answers\n",
    "## TODO - this is a hacky way to do this. Should be a better way.\n",
    "df['cot_answer'] = df.apply(lambda x: extract(x['cot_result']), axis=1)\n",
    "df['cot_pred'] = df.cot_answer.map(target_numbers)\n",
    "\n",
    "## Get token counts\n",
    "df[\"cot_completion_tokens\"] = df.cot_result.apply(lambda x: len(encoding.encode(x)))\n",
    "df[\"cot_prompt_tokens\"] = df.cot_input.apply(lambda x: len(encoding.encode(x)))\n",
    "\n",
    "## Calculate performance stats\n",
    "y_true = df['ground_truth']\n",
    "y_pred = df['cot_pred']\n",
    "report = classification_report(y_true, y_pred,target_names=target_names.values(), labels=list(target_names.keys()), zero_division=1, output_dict=True)\n",
    "\n",
    "### Collect statistics\n",
    "cot_stats = {\n",
    "    \"precision\": report['weighted avg']['precision'],\n",
    "    \"recall\": report['weighted avg']['recall'],\n",
    "    \"f1\":  report['weighted avg']['f1-score'],\n",
    "    \"support\": report['weighted avg']['support'],\n",
    "    \"completion_tokens\": df[\"cot_completion_tokens\"].mean(),\n",
    "    \"prompt_tokens\": df[\"cot_prompt_tokens\"].mean(),\n",
    "    \"latency\": 4.22 ## Taken directly from TQDM progress bar\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>support</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cot_stats</th>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>133.81</td>\n",
       "      <td>141.99</td>\n",
       "      <td>4.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>null_stats</th>\n",
       "      <td>0.749054</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.749451</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8.69</td>\n",
       "      <td>107.99</td>\n",
       "      <td>1.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Let's display a table of the stats before doing any analysis.\n",
    "from IPython.display import display, HTML\n",
    "data = {\"cot_stats\": cot_stats, \"null_stats\": null_stats}\n",
    "results = pd.DataFrame(data).transpose()\n",
    "\n",
    "# Display the HTML table\n",
    "display(HTML(results.to_html()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh-oh, results are actually worse. Are they signicantly worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "McNemar's test p-value: 0.629058837890625\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "# Perform McNemar's test\n",
    "contingency_table = pd.crosstab(df['null_pred'] == df['ground_truth'], df['cot_pred'] == df['ground_truth'])\n",
    "result = mcnemar(contingency_table, exact=True)\n",
    "\n",
    "print(f\"McNemar's test p-value: {result.pvalue}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The COT prompt achieved a precision of 0.79, recall of 0.78, and F1 score of 0.78 on the 100 ABA ethics questions tested. In comparison, the null prompt had a precision of 0.73, recall of 0.73, and F1 score of 0.73. Responses to the COT prompt contained an average of 141 tokens, took an average of 4.27 seconds to generate, and required max of 289 tokens. The null prompt received responses with an average of 8 tokens, took 1.12 seconds on average, and used a maximum of 10 tokens.\n",
    "\n",
    "### Key Takeaways\n",
    "* The COT prompt achieved higher precision, recall, and F1 score compared to the null prompt, indicating a potential advantage in eliciting correct responses.\n",
    "* Responses to the COT prompt contained significantly more tokens on average than those to the null prompt, suggesting that the COT prompt encouraged more detailed and reasoning-based responses.\n",
    "* The COT prompt exhibited over 3x higher latency compared to the null prompt, which can be attributed to the additional cognitive load required to produce thoughtful responses.\n",
    "* Both prompts had nearly the same number of prompt tokens on average, indicating that the difference in response quality is due to the prompt style rather than length.\n",
    "* The model's performance on both prompts was moderately high, providing a solid baseline for future experiments.\n",
    "\n",
    "However, the McNemar's test p-value of 0.30 suggests that the difference in classifier performance is not statistically significant. Based on these results, the official verdict is **Plausible**. It is recommended to collect more examples to conclusively determine the impact of the Chain of Thought (COT) prompt on performance.\n",
    "\n",
    "In conclusion, while the effectiveness of the COT prompt has been well documented in the literature, this exercise aimed to apply a methodical approach to comparing prompt performance. The results highlight that although the performance is slightly better, it comes with significant costs. Therefore, a prudent prompt engineer should carefully consider the trade-offs when applying techniques, as they can have a substantial impact on cost with a statistically insignificant effect on performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The COT prompt achieved an F1 score of 0.72 on the 100 ABA ethics questions tested. In comparison, the null prompt had an F1 score of 0.75. Responses to the COT prompt contained an average of 133 tokens, took an average of 4.22 seconds to generate, and required max of 283 tokens. The null prompt received responses with an average of 8.6 tokens, took 1.15 seconds on average, and used a maximum of 9 tokens. \n",
    "\n",
    "### Key Takeaways \n",
    "* The null prompt achieved slightly higher precision, recall, and F1 score compared to the COT prompt, indicating a potential disadvantage for the COT prompt in eliciting correct responses on this dataset.\n",
    "* Responses to the COT prompt contained significantly more tokens on average than those to the null prompt, meaning Chain of thought prompting costs about 2.3 times more tokens (prompt + completion) on datasets of this complexity.\n",
    "* The COT prompt exhibited over 3x higher latency compared to the null prompt, which can be attributed to the additional tokens requested.\n",
    "* The model's performance on both prompts was moderately high, providing a solid baseline for future experiments. However, the McNemar's test p-value of 0.63 suggests that the difference in classifier performance is not statistically significant.\n",
    "\n",
    "Based on these results, the official verdict is **Busted**. However, this does not contradict of invalidate the results of the many published papers on this topic. Instead, we simply proved that for this specific dataset, the Chain of Thought could not ellicit better answers despite seemingly sound reasoning and expecting benefits from thsi technique. In conclusion, while the effectiveness of the COT prompt has been well documented in the literature, this exercise aimed to apply a  approach to comparing prompt performance. The results highlight that on some tasks chain of thought may be a slight determinent or at the very least cost signficantly more for non-significant impovements. Therefore, a prudent prompt engineer should carefully consider the trade-offs when applying techniques, as they can have a substantial impact on cost with a statistically insignificant effect on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>null_completion_tokens</th>\n",
       "      <th>cot_completion_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.690000</td>\n",
       "      <td>133.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.464823</td>\n",
       "      <td>49.503126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>102.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>125.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>159.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>283.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       null_completion_tokens  cot_completion_tokens\n",
       "count              100.000000             100.000000\n",
       "mean                 8.690000             133.810000\n",
       "std                  0.464823              49.503126\n",
       "min                  8.000000              47.000000\n",
       "25%                  8.000000             102.750000\n",
       "50%                  9.000000             125.500000\n",
       "75%                  9.000000             159.250000\n",
       "max                  9.000000             283.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"null_completion_tokens\", \"cot_completion_tokens\"]].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other issue I encountered with using a prompt like this is its highly variable output length. While the null_prompt produced a consistent 32-word response, the CoT prompt ranged from 45 to 289 tokens. This variability is dangerous not just due to cost, but because in this study I performed chain of thought in a single inference. If I had limited the maximum tokens, I may have exceeded the limit before providing an answer.\n",
    "\n",
    "In the future, I plan to test performing multiple inferences for chain of thought to compare the costs and benefits. On the one hand, multiple inferences might guarantee a successful response. But it remains to be seen whether the additional inference time and tokens required would be worth it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revision 1\n",
    "I noticed alot of typos, especially in the prompts that were applied once I got a fresh set of eyes this morning. The edits I have made were only to comply with the original metholodogy set out. I was just fixing some bugs. Please checkout the git history to see what typos were fixed and how they impacted performance.\n",
    "\n",
    "Unfortunately, these corrections had a major chane on the results that invalidated the entire analysis!!!! Since I had to make changes I'm going to consider this a soft bust only. Obviously chain of thought is a useful technique for some kinds of reasoning tasks. This analysis does show that not all situations benefit from it though. We should be selective in our use of prompt techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Asisstant: <assistant_instructions>\n",
      "    <role>\n",
      "        Correctly Answer True/False questions about the ABA Model Rules of Professional Conduct. Before providing an answer, think through each step of your reasoning.\n",
      "    </role>\n",
      "    <output_format>\n",
      "        <reasoning>\n",
      "            {scratch pad for reasoning through the question}\n",
      "        </reasoning>\n",
      "        <answer>\n",
      "            {True or False}\n",
      "        </answer>\n",
      "    </output_format>\n",
      "</assistant_instructions>\n",
      "\n",
      "Human: <input>\n",
      "    A parent who pays for the lawyer's services on behalf of a child may direct or control some of the legal strategy decisions made.\n",
      "</input>\n",
      "\n",
      "Assistant: <output>\n",
      "    <reasoning>\n",
      "\n",
      "<reasoning>\n",
      "The ABA Model Rules state that a lawyer shall abide by a client's decisions concerning the objectives of representation and shall consult with the client as to the means by which they are to be pursued. (Rule 1.2(a))\n",
      "The lawyer shall not accept compensation for representing a client from one other than the client unless: (1) the client gives informed consent; (2) there is no interference with the lawyer's independence of professional judgment or with the client-lawyer relationship; and (3) information relating to representation of a client is protected as required by Rule 1.6. (Rule 1.8(f))\n",
      "In this scenario, the parent is paying for the legal services but the child is the actual client. As long as the lawyer obtains the child's informed consent, ensures there is no interference with their independent judgment, and protects the child's confidential information, the parent may direct some strategy decisions. However, the child's objectives and interests must remain paramount. \n",
      "</reasoning>\n",
      "<answer>\n",
      "    True\n",
      "</answer>\n",
      "</output>\n"
     ]
    }
   ],
   "source": [
    "## Let's take a look at the COT results that were wrong\n",
    "sample  = df[df['cot_pred'] != df['ground_truth']].sample(1)\n",
    "print(sample.cot_input.values[0])\n",
    "print(sample.cot_result.values[0])\n",
    "\n",
    "## I'm not a lawyer but the reasoning seems sound except there are some subtle fallacy's of hallunications being introduced that lead the LLM astray. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
